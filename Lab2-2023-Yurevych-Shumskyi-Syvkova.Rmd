---
title: 'P&S-2022: Lab assignment 2'
author: "Шумський Дмитро (завдання 3), Юревич Наталія (завдання 2), Сивкова Анна (завдання 1)"
output:
  html_document:
    df_print: paged
---

The work is done by:

Syvkova Anna (task 1),

Yurevych Nataliia (task 2),

Shumskyi Dmytro (task 3)

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the confidence interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# Set the team ID to 32
id <- 32

# Set the random seed based on the team ID
set.seed(id)

# Define the probability p based on the team ID
p <- id / 100

# Define the generator matrix G
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
              1, 0, 0, 1, 1, 0, 0,
              0, 1, 0, 1, 0, 1, 0,
              1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)

# Define the parity-check matrix H
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
              0, 1, 1, 0, 0, 1, 1,
              0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))

# Print the matrices if needed
cat("Matrix G:\n")
print(G)
# G
cat("The matrix H is: \n") 
print(H)
cat("The product GH must be zero: \n")
result <- (G %*% H) %% 2
print(result)
```

#### Next, generate the messages

```{r}
# Function to generate N random 4-bit messages
# Args:
#   N: Number of messages to generate
# Returns:
#   A matrix where each row represents a 4-bit message
N <- 1000
message_generator <- function(N) {
  # Generate a matrix of random 0s and 1s to represent messages
  # Each message is a 4-bit binary sequence
  messages <- matrix(sample(c(0, 1), 4 * N, replace = TRUE), nrow = N)
  
  # Return the matrix of generated messages
  return(messages)
}

# Example usage:
# Generate 100 random 4-bit messages
messages <- message_generator(N)
# 'codewords' contains the 7-bit codewords resulting from encoding the 4-bit messages
# using the generator matrix G of the [7,4] Hamming code. These codewords are designed
# for reliable data transmission with error detection and correction capabilities.
# Each row of 'codewords' represents an encoded message ready for transmission.

codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
# Set the probability of errors
p <- 0.32  # Replace with your desired error probability

# generate_errors is a function that generates random errors based on the given probability p. It iterates through each element of a matrix (in this case, the codewords matrix) and introduces errors with a probability p. A value of 1 represents an error, and 0 represents no error.
generate_errors <- function(matrix, p) {
 errors <- matrix(0, nrow(matrix), ncol(matrix))  # Initialize an errors matrix
 for (i in 1:nrow(matrix)) {
    for (j in 1:ncol(matrix)) {
      # Generate a random number and introduce an error with probability p
      if (runif(1) < p) {
        errors[i, j] <- 1  # 1 represents an error
      }
    }
  }
  return(errors)
}

# Generate errors based on the probability p
errors <- generate_errors(codewords, p)

# transmit_messages is a function that simulates the transmission of codewords by adding the generated errors. It takes the original codewords and the errors as input, applies the modulo-2 operation, and returns the received messages.

transmit_messages <- function(codewords, errors) {
  received <- (codewords + errors) %% 2
  return(received)
}

# Generate received messages by simulating transmission
received <- transmit_messages(codewords, errors)
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages.

```{r}
decoded_messages <- matrix(0, nrow(received), 4)
# Iterate through each received message and calculate the syndrome vector
for (i in 1:nrow(received)) {
  # Extract the i-th received message
  received_message <- received[i, ]

  # Calculate the syndrome vector for the i-th received message
  syndrome <- (received_message %*% H) %% 2
  

  # Check if errors have occurred
  if (sum(syndrome) > 0) {
    # Error correction: Identify the position of the error
    error_position <- syndrome[1] + 2 * syndrome[2] + 4 * syndrome[3]

    # Correct the error
        received[i, error_position] <- 1 - received[i, error_position]
  }

  # Decode the received message
  decoded_message <- received[i,c(3,5,6,7)]
  decoded_messages[i, ] <- decoded_message
}
```

Comparing initial messages with decoded

```{r}
# Assuming 'messages_matrix' is your 100x4 matrix of messages, and 'decoded_matrix' is your 100x4 matrix of decoded messages

# Initialize a counter for the number of matching pairs
matching_pairs <- sum(apply(decoded_messages - messages, 1, sum) == 0)
probability_correct_decoding <- matching_pairs/N
check <-  (1 - p)^4 + p*(1-p)^3 # The 'check' variable is used to estimate the probability of correctly receiving
# a 4-bit message. It considers two scenarios:
# 1. (1 - p)^4: The probability that all four bits remain uncorrupted,
# indicating no errors in the 4-bit message.
# 2. p * (1 - p)^3: The probability that exactly one bit is corrupted (single error)
# while the other three bits remain uncorrupted.

cat("Number of matching pairs:", matching_pairs, "\n")
cat("Probability of correct decoding:", probability_correct_decoding, "\n")
cat("Probability of correct decoding p*", check, "\n")
```

1.  In summary, for large N, the estimate \$\\hat p\$ is expected to be close to \$p\^*\$ because the law of large numbers ensures that, on average, the estimate becomes more accurate with a larger number of trials. Additionally, the assumption of independent and identically distributed trials, along with the consistency of the process, supports the convergence of \$\\hat p\$ to \$p\^*\$ as N grows.

    **By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the** confidence **interval** $(p^*-\varepsilon, p^* + \varepsilon)$**, in which the estimate** $\hat p$ **falls with probability at least** $0.95$**.**

```{r}
z_score <- qnorm(0.975) # 95% confidence level

# Calculate the standard error
SE <- sqrt(probability_correct_decoding * (1 - probability_correct_decoding) / N)

# Calculate the margin of error
margin_of_error <- z_score * SE

# Calculate the confidence interval
confidence_interval <- c(probability_correct_decoding - margin_of_error, probability_correct_decoding + margin_of_error)

cat("Margin of error:", margin_of_error)
cat("95% Confidence Interval:", confidence_interval, "\n")

```

```{r}
# Assuming 'probability_correct_decoding' is the estimated probability of correct decoding
epsilon <- 0.03 # Margin of error

# Calculate the minimum required sample size
min_sample_size <- ceiling((z_score^2 * probability_correct_decoding * (1 - probability_correct_decoding)) / epsilon^2)

cat("Minimum required sample size:", min_sample_size, "\n")

```

```{r}
# Calculate the absolute differences between decoded messages and original messages
absolute_diff = abs(decoded_messages - messages)

# The 'absolute_diff' variable contains the absolute differences between each bit
# in the decoded messages and the corresponding bits in the original messages.
# This helps quantify the errors in each decoded message.

# Calculate the total number of errors in each decoded message
num_errors <- apply(absolute_diff, 1, sum)

# The 'num_errors' variable represents the total number of errors in each decoded message.
# It sums up the absolute differences, giving the count of errors in each message.

# Create a histogram to visualize the distribution of error counts
hist(num_errors, breaks = seq(-0.5, 4.5, by = 1), 
     main = "Number of Errors",
     xlab = "Number of Errors", 
     ylab = "Frequency",
     xlim = c(0, 4))

# This code generates a histogram that shows the distribution of the counts of errors
# in the decoded messages. The histogram helps analyze how frequently different
# numbers of errors occur in the decoding process, allowing an assessment of the
# error-correcting capability of the code.

```

The distribution of errors in this case is indeed a binomial distribution because it models the number of errors (failures) in a series of independent binary bit transmissions.

-   Each binary bit transmission can be considered a Bernoulli trial with two possible outcomes: success (bit transmitted correctly) with probability (1 - p_error) and failure (bit transmitted incorrectly) with probability p_error.

-   You are interested in the number of successful transmissions (k) in a fixed number of trials (4 trials), which fits the definition of a binomial distribution.

**Conslusion:**

-   The estimate ˆp of the probability p\* of correct transmission becomes more accurate as N (the number of trials) increases, thanks to the Law of Large Numbers.

-   The choice of N to ensure ε ≤ 0.03 in a confidence interval can be determined by solving the equation involving the standard error, sample size, and critical value.

-   The histogram of the number of errors in a 4-digit binary message follows a binomial distribution, a well-known distribution for counting the number of successes in a fixed number of independent trials with the same probability of success.

-   The key to the accuracy of these estimations and the use of the CLT is the assumption of independent trials and a constant probability of success/error in each trial in the encoding-transmission-decoding process.

### Task 2.

#### In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

#### Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

#### Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the c.d.f. $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the e.c.d.f. $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two c.d.f.'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

The parameter of Poisson distribution $X_i$ is $\mu:=N\lambda$. The number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds. We have $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each.

### Task 2. Part 1.

```{r}
set.seed(32)

# const
m <- 32 * 10^(-6)
T_years <- 30.1
molar_mass <- 136.9070895
seconds_per_year <- 31557600
avogadro_const <- 6 * 10^23

# calculate
lambda <- log(2, base=exp(1)) / (T_years * seconds_per_year)
N <- (m / molar_mass) * avogadro_const
mu_poisson <- N * lambda

```

### Task 2. Part 2.

#### Next, calculate the parameters of the standard normal approximation

```{r}
set.seed(32)

K <- 1e3
# n <- 1
# n <- 5
# n <- 10
# n <- 50

for (n in c(1, 5, 10, 50)) {
  sampe_mean <- mean(rpois(n*K, lambda = mu_poisson))
  
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu_poisson), nrow=n)) # a vector with the mean values for each column (piece of material)
  
  # Identify μ and σ² for the normal distribution
  mean_norm <- mean(sample_means)
  sd_norm <- sd(sample_means)
  
  # Generate the normal distribution c.d.f.
  cdf_norm <- pnorm(n*K, mean_norm, sd_norm)
  
  # Compute the empirical cumulative distribution function (e.c.d.f.)
  ecdf_ <- ecdf(sample_means)
  
  # Calculate the maximal difference
  x_values <- seq(mean_norm - 3 * sd_norm, mean_norm + 3 * sd_norm, length.out = 1000)
  cdf_xval <- pnorm(x_values, mean = mean_norm, sd = sd_norm)
  ecdf_xval <- ecdf_(x_values)
  
  differences <- abs(cdf_xval - ecdf_xval)
  max_difference <- max(differences)
  mean_difference <- mean(differences)
  
  xlims <- c(mean_norm - 3 * sd_norm, mean_norm + 3 * sd_norm) # center the plot around the mean
  plot(ecdf_, 
       xlim = xlims,
       ylim = c(0,1),
       col = "blue",
       lwd = 2,
       main = sprintf("Comparison of ecdf and cdf (n = %d)", n))
  curve(pnorm(x, mean_norm, sd_norm), col = "red", lwd = 2, add = TRUE) # adds the CDF of a normal distribution N(mu, sigma)
  
  print("Max. difference:")
  print(max_difference)
  print("Mean difference:")
  print(mean_difference)
}

```

### Task 2. Part 3.

```{r}
critical_value <- 8e8
max_n <- 50
K <- 1e3
max_n_below_critical_value <- 0
for (j in 1:max_n) {
    n <- j
    sum <- colSums(matrix(rpois(n * K, lambda = mu_poisson), nrow = n))
    emp_probability <- length(sum[sum < critical_value]) / K
    if (emp_probability > 0.95) {
      max_n_below_critical_value <- n
    }
}
print("Maximum n (before getting the critical value):")
print(max_n_below_critical_value)
```

### Markov's inequality

$$P(X \geq a) \leq \frac{E(X)}{a} $$

We need to find n for such formula: $$ P(\sum_{i=1}^{n}{Xi} < 8*10^8) \geq 0.95 $$

$$ P(\sum_{i=1}^{n}{Xi} < 8*10^8) \geq 0.95 = 1 - P(\sum_{i=1}^{n}{Xi} \geq 8*10^8) \geq 0.95 = P(\sum_{i=1}^{n}{Xi} \geq 8*10^8) \leq 0.05 $$ $$ \frac{E(x)}{8*10^8}=\frac{n\mu}{8*10^8} $$ Now we can calculate n: $$ n = \frac {102336296862000}{102336436265707} \approx  0,999999$$

### Chernoff

Chernoff bound is $P(X \ge a) \le min_{t>0}(\frac{M_x(t)}{e^{ta}})$

Let $$S = \sum_{i=1}^{n}X_i=>M_s(t)=E(e^{ntX_1})=e^{n\lambda(e^t-1)}$$ as $X_i$ are i. i. d.

$\lambda$ - parameter of Poisson distribution.

$min(\frac{M_x(t)}{e^{ta}})=min(e^{n\lambda(e^t-1)-ta})$. $e^x$ is increasing, function we need to find $f(t)=min(n\lambda(e^t-1)-ta)$

Let's use a derivative: $\frac{df}{dt}=n\lambda e^t-a$

$n\lambda e^t-a=0=>e^t=\frac{a}{n\lambda}$

So, $f(t)=a-n\lambda-aln(\frac{a}{n\lambda})=>min(\frac{M_x(t)}{e^{ta}})=e^{a-n\lambda}(\frac{n\lambda}{a})^a$

$P(S\ge a)\le e^{a-n\lambda}(\frac{n\lambda}{a})^a\equiv 1-P(S\le a-1)\le e^{a-n\lambda}(\frac{n\lambda}{a})^a\equiv P(s \le a-1)\ge1-e^{a-n\lambda}(\frac{n\lambda}{a})^a$

So, $a = 8*10^8 + 1$ and $1-e^{a-n\lambda}(\frac{n\lambda}{a})^a=0.95=>e^{a-n\lambda}(\frac{n\lambda}{a})^a=0.05$

As $a$ is large enough, $e^{a-n\lambda}(\frac{n\lambda}{a})^a=0.05=>e^{1-\frac{n\lambda}{a}}$

So, $\frac{n\lambda}{a}=0.05^{\frac{1}{a}}\approx1$ $\frac{n\lambda}{a}\approx1=>n=\frac{a}{\lambda}=\frac{8*10^8}{102336436,265707}\approx7.81735$

And we get $n\le 7$

### CLT

$P(\frac{Sn - n\mu}{\sigma \sqrt{n}} \leq t) = \phi (t)$

$P(\frac{Sn - n\mu}{\sigma \sqrt{n}} \geq 0.95) = 1- \phi (0.95)$

$1 - \phi (t) \approx 0.171056$

$\frac{8*10^8 - n * 102336426,7943}{ \sqrt{n}*1408,60281805265} \approx 7.817353263740834$

$n \approx 7.86811$

So, $n \le 7$.

### Conclusions:

СLT and Chernoff has the highest precition.

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the $$\textbf{r.v.}$$ $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the empirical cumulative distribution function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the c.d.f. of $\mathscr{N}(\mu,\sigma^2)$ is close to the e.c.d.f. $F_{\mathbf{s}}$ of and plot both c.d.f.'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two c.d.f.'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the r.v. $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

## Part 1

### First, generate samples an sample means:

```{r}
set.seed(32)
team_number = 32
nu1 <- team_number + 10
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
```

### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 1 / nu1
sigma <- 1 / (sqrt(n) * nu1)
```

### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

### Calculate the maximal difference between the two c.d.f.'s;

```{r}
# Calculate the maximal difference between the two CDFs
x <- seq(0,.1,by=.0001)
Fs <- ecdf(sample_means)
max_difference = max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma)))

# Print the maximal difference
cat("Maximal Difference:", max_difference, "\n")

```

### Consider cases $n = 5$, $n = 10$, $n=50$

```{r}
calc_cases <- function(n) {
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
  sigma <- mu / sqrt(n)
  
  x <- seq(0,.1,by=.0001)
  Fs <- ecdf(sample_means)
  max_difference = max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma)))
  
  return (max_difference)
}

cat("n = 5: ", calc_cases(5), "\n")
cat("n = 10: ", calc_cases(10), "\n")
cat("n = 50: ", calc_cases(50), "\n")

```

## Part 2

### Express the event of interest in terms of the r.v. $S:= X_1 + \cdots + X_{100}$;

$$P(S \geq 60) \geq 0.95$$

### Obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;

$$
E(S) = E(X_1 + \ldots + X_{100}) = E(X_1) + E(X_2) + \ldots =
100 \cdot E(X_i) = \frac{100}{\lambda \cdot N} = \frac{100}{42 \cdot N} \\
$$

1)  $$        $$ $$ Markov\ ineqality$$

$$
0.95 \leq P(S \geq 1) \leq \frac{100}{42 \cdot N}\\0.95 \leq \frac{100}{42 \cdot N}\\N \leq 2.5
$$

2)  $$        $$ $$ Chernoff\ bound$$

$$\mathbb{P}(X \geq a) \leq e^{-at}M_X(t)$$

$$\mathbb{P}(X \geq a) \leq e^{-at}\left(\frac{\lambda N}{\lambda N - t}\right)^{100}\;\;\;\;\;\;\;\; 0 \leq t \leq \lambda N$$

$$M_s(t) = E(e^{st}) = E(e^{(X_1 + X_2 + \ldots + X_{100})t}) = E(e^{x_1t} \cdot e^{x_2t} \cdot \ldots \cdot e^{x_{100}t}) = (E(e^{x_i t}))^{100} = (M_x(t))^{100} = \left(\frac{\lambda N}{\lambda N - t}\right)^{100}$$

$$0.95 \leq P(S \geq 1) \leq e^{-t} \left(\frac{\lambda N}{\lambda N - t}\right)^{100}$$

$$0.95 \leq e^{-t} \left(\frac{\lambda N}{\lambda N - t}\right)^{100}$$

$$We\ need\ to\ get\ N,\ so\ after\ some\ calculations,\ we\ get:$$

$$N \leq -\frac{t \cdot (0.95e^t)^{1/100}}{42 \cdot \left(1 - (0.95e^t)^{1/100}\right)}$$

$$
After\ minimizing\ we\ get:
$$

$$
N \leq 2.45
$$

3)  $$        $$ $$ Central\ limit\ theorem$$$$ P\left(\frac{X_1 + \ldots + X_{100} - \mu}{\sigma\sqrt{n}} \leq\frac{1-n\mu}{\sigma\sqrt{n}}\right) \rightarrow \Phi\left(\frac{1-n\mu}{\sigma\sqrt{n}}\right)\\ so\ it \ is\  eqal\ to\\ \Phi\left(\frac{1-\frac{100}{42N}}{\frac{10}{42N}}\right)$$

### With the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;

```{r}
sum(rexp(100, rate=2*42))

```

### Repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;

```{r}
# Let K = 100000
row_sum = rowSums(matrix(rexp(100*100000, rate = 2*42), nrow=100000))
```

### Estimate the probability that the location is identified as safe and compare to the desired level $0.95$

```{r}
row_sum_ecdf <- ecdf(row_sum)
safe <- 1 - row_sum_ecdf(1)
cat("Probability that the location is safe", safe)

```

### General summary and conclusions

In Task 3, the Central Limit Theorem is used to analyze the distribution of sample means of exponentially distributed random variables, which model the time between clicks of a Geiger counter measuring radioactivity. The goal is to determine the distribution of sample means, their proximity to a normal distribution, and to estimate the maximum number of radioactive samples that can be stored in a location while maintaining safety.

The results of these simulations and calculations provide insights into the behavior of the radioactive samples, the safety of the location, and how sample means approximate a normal distribution as \$n\$ increases.

The provided R code includes the actual implementation of these steps, and the output of each part can be observed through this code.
